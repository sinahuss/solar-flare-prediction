{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sinahuss/solar-flare-prediction/blob/main/notebooks/solar_flare_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "### Note for Colab Users\n",
    "\n",
    "- **Quick exploration**: Review the pre-computed outputs without re-running\n",
    "- **For full execution**: Run all cells (~8 minutes on Colab CPU)\n",
    "- **Prediction interface**: All cells must be fully executed, then jump to Section 6.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkTkq0gHjNlg"
   },
   "source": [
    "# **C964 Capstone: Solar Flare Prediction and Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Organizational Need\n",
    "\n",
    "Space weather events, particularly solar flares, pose significant risks to critical infrastructure on Earth and in space. Organizations like NOAA's Space Weather Prediction Center require reliable early warning systems to protect:\n",
    "\n",
    "- Satellite communications and GPS systems\n",
    "- Power grids\n",
    "- Astronauts and aircraft\n",
    "- Radio communications\n",
    "\n",
    "Current prediction methods rely heavily on human expertise and limited historical patterns, which may result in missed events or false alarms. These risks can lead to potentially billions of dollars in economic damage and disruptions to essential services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Project Goal\n",
    "\n",
    "This project aims to develop a data product featuring a machine learning model that can predict the likelihood of solar flare events (C, M, or X class) within a 24-hour period based on characteristics of sunspot regions. The model will provide early warning capability for space weather forecasters, and improved accuracy in flare prediction to reduce false alarms and missed events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Success Criteria\n",
    "\n",
    "Missing an X-class flare (false negative) is far more costly than incorrectly predicting one (false positive), so at least 60% recall for severe flares is our primary optimization target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-2r8JEfeptP"
   },
   "source": [
    "### 2.1. Load Libraries and Data\n",
    "\n",
    "Our solar flare prediction analysis begins with importing essential libraries and loading the sunspot dataset.\n",
    "\n",
    "The dataset will be loaded from a public GitHub repository containing the Solar Flare Dataset from Kaggle, which provides the historical data needed to train our flare prediction model.\n",
    "\n",
    "This dataset contains morphological characteristics of sunspot groups that solar physicists use to assess flare potential. The first few rows will be displayed to verify successful data loading and provide an initial glimpse of the sunspot characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCBbIizzO1Mu"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "from matplotlib import (\n",
    "    pyplot as plt,\n",
    "    cm,\n",
    ")\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine learning preprocessing and model selection\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Machine learning algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Model evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# Handling imbalanced datasets\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Model interpretability\n",
    "import shap\n",
    "\n",
    "# Import ipywidgets for interactive interface\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset from GitHub repository (public Kaggle dataset)\n",
    "url = \"https://raw.githubusercontent.com/sinahuss/solar-flare-prediction/refs/heads/main/data/data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display first few rows to verify successful loading\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dataset Feature Descriptions\n",
    "\n",
    "The dataset contains 13 features describing each solar active region. The first 10 are the input features for our model, and the last three are the target variables we aim to predict.\n",
    "\n",
    "**Input Features:**\n",
    "\n",
    "- `modified Zurich class`: A classification of the sunspot group's magnetic complexity, generally ordered from least to most complex (A, B, C, D, E, F, H).\n",
    "- `largest spot size`: Size of the largest spot in the group, ordered from smallest to largest (X, R, S, A, H, K).\n",
    "- `spot distribution`: Compactness of the sunspot group, ordered from least to most compact (X, O, I, C).\n",
    "- `activity`: A code representing the region's recent growth (1=decay, 2=no change).\n",
    "- `evolution`: Describes the region's evolution over the last 24 hours (1=decay, 2=no growth, 3=growth).\n",
    "- `previous 24 hour flare activity`: A code summarizing prior flare activity (1=none, 2=one M1, 3=>one M1).\n",
    "- `historically-complex`: A flag indicating if the region was ever historically complex (1=Yes, 2=No).\n",
    "- `became complex on this pass`: A flag indicating if the region became complex on its current transit (1=Yes, 2=No).\n",
    "- `area`: A code for the total area of the sunspot group (1=small, 2=large).\n",
    "- `area of largest spot`: A code for the area of the largest individual spot (1=<=5, 2=>5).\n",
    "\n",
    "Target Variables:\n",
    "\n",
    "- `common flares`: The number of C-class flares produced in the next 24 hours.\n",
    "- `moderate flares`: The number of M-class flares produced in the next 24 hours.\n",
    "- `severe flares`: The number of X-class flares produced in the next 24 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRY4P9xGI94V"
   },
   "source": [
    "### 2.3. Initial Data Inspection\n",
    "\n",
    "A foundational understanding of the dataset's structure and quality must be established. This inspection is critical for the solar flare prediction model because data quality directly impacts model performance and reliability for space weather forecasting.\n",
    "\n",
    "First, we will use `.info()` to examine the column names, data types, and check for any missing values. The output confirms that there are no missing values, meaning that null values do not have to be accounted for in the data preparation phase.\n",
    "\n",
    "Next, we use `describe()` to generate a summary of the categorical features, including their unique values and most frequent entries, which helps us understand the distribution and composition of the dataset's categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmC34kT4I-sO"
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.astype(\"object\").describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1. Target Variable Analysis\n",
    "\n",
    "Before analyzing the input features, we must first understand the distribution of our target variables: `common flares`, `moderate flares`, and `severe flares`. The plots below show the number of 24-hour periods in the dataset that recorded zero, one, or more flares of each type.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "The visualization reveals a severe class imbalance for our solar flare prediction. Out of all 24-hour periods available, only 16% experienced at least one C-Class event, 5% recorded M-Class events, and 1% showed X-Class events.\n",
    "\n",
    "This imbalance has implications for our machine learning approach:\n",
    "\n",
    "- **Sampling Strategy:** We may need to employ techniques like stratified sampling and SMOTE (Synthetic Minority Oversampling Technique) to address the imbalance.\n",
    "\n",
    "- **Evaluation Metrics:** The model's success will be measured primarily by the macro f1 score rather than overall accuracy, because it places greater emphasis on rare X class flares, which are the most impactful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_columns = [\"common flares\", \"moderate flares\", \"severe flares\"]\n",
    "\n",
    "# Create a figure with 3 subplots, one for each flare type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "fig.suptitle(\"Distribution of Raw Flare Counts Per 24-Hour Period\")\n",
    "\n",
    "# Loop through each flare type and plot its distribution\n",
    "for i, col in enumerate(flare_columns):\n",
    "    ax = axes[i]\n",
    "    countplot = sns.countplot(\n",
    "        data=df, x=col, ax=ax, hue=col, palette=\"viridis\", legend=False\n",
    "    )\n",
    "    ax.set_title(f\"{col}\")\n",
    "    ax.set_xlabel(\"Flares Recorded\")\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%d\", label_type=\"edge\", padding=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Relationship Analysis\n",
    "\n",
    "We now explore the relationship between flare production and `modified Zurich class` through two visualizations. These analyses investigate a key hypothesis: that more complex sunspot groups produce more significant flares.\n",
    "\n",
    "**Total Flares Analysis:** The first subplot shows the total number of C, M, and X-class flares produced by each modified Zurich class, revealing which sunspot configurations are the most prolific sources of solar flares.\n",
    "\n",
    "**Average Flares Analysis:** The second subplot normalizes this data by showing the average number of flares per class instance, accounting for the different frequencies of each modified Zurich class in the dataset. This provides a more accurate assessment of flare risk per sunspot group.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "The two visualizations help us prioritize which modified Zurich class to observe.\n",
    "\n",
    "- **Low-Risk:** B and C class sunspot regions are low complexity and produce the least amount of solar flares, so they can be seen as low-risk regions. H class regions are decayed remnants of C, D, E, and F regions, and are also low-risk regions.\n",
    "\n",
    "- **Medium-Risk:** D class sunspot regions are interesting because they produce the highest number of total solar flares in the dataset. But, after normalizing the data, we can see that they actually produce significantly fewer flares per sunspot region. Therefore, they can be categorized as medium-risk regions.\n",
    "\n",
    "- **High-Risk:** E class regions are almost guaranteed to produce solar flares, reaching just under 1 C-Class solar flare per instance. F class regions produce a low total amount of solar flares, but adjusting for their lower representation in the dataset, they produce a high number of solar flares per region. F class regions also produce the highest amount of X-class (severe) flares when data is normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to have a single column for flare type and another for the count\n",
    "flare_counts_df = df.melt(\n",
    "    id_vars=[\"modified Zurich class\"],\n",
    "    value_vars=[\"common flares\", \"moderate flares\", \"severe flares\"],\n",
    "    var_name=\"flare_type\",\n",
    "    value_name=\"count\",\n",
    ")\n",
    "\n",
    "# Specify the order for each categorical feature for consistent plotting\n",
    "category_orders = {\n",
    "    \"modified Zurich class\": [\"B\", \"C\", \"D\", \"E\", \"F\", \"H\"],\n",
    "    \"largest spot size\": [\"X\", \"R\", \"S\", \"A\", \"H\", \"K\"],\n",
    "    \"spot distribution\": [\"X\", \"O\", \"I\", \"C\"],\n",
    "}\n",
    "\n",
    "# Remove rows where flares have not occurred\n",
    "flare_counts_df = flare_counts_df[flare_counts_df[\"count\"] > 0]\n",
    "\n",
    "# Calculate the number of sunspot groups for each Zurich class\n",
    "zurich_class_counts = df[\"modified Zurich class\"].value_counts().to_dict()\n",
    "\n",
    "# Calculate the proportional number of flares (per Zurich class instance)\n",
    "flare_counts_df[\"class_count\"] = flare_counts_df[\"modified Zurich class\"].map(\n",
    "    zurich_class_counts\n",
    ")\n",
    "flare_counts_df[\"count_per_class\"] = (\n",
    "    flare_counts_df[\"count\"] / flare_counts_df[\"class_count\"]\n",
    ")\n",
    "\n",
    "# Create a figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Create the Grouped Bar Plot\n",
    "sns.barplot(\n",
    "    data=flare_counts_df,\n",
    "    x=\"modified Zurich class\",\n",
    "    y=\"count\",\n",
    "    hue=\"flare_type\",\n",
    "    estimator=sum,\n",
    "    order=category_orders[\"modified Zurich class\"],\n",
    "    palette=\"viridis\",\n",
    "    errorbar=None,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Total Flares Produced by Sunspot Zurich Class\")\n",
    "ax1.set_xlabel(\"Modified Zurich Class\")\n",
    "ax1.set_ylabel(\"Total Number of Flares Recorded\")\n",
    "ax1.legend(title=\"Flare Type\")\n",
    "\n",
    "# Second subplot: Average Flares per Class Instance\n",
    "sns.barplot(\n",
    "    data=flare_counts_df,\n",
    "    x=\"modified Zurich class\",\n",
    "    y=\"count_per_class\",\n",
    "    hue=\"flare_type\",\n",
    "    estimator=sum,\n",
    "    order=category_orders[\"modified Zurich class\"],\n",
    "    palette=\"viridis\",\n",
    "    errorbar=None,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Average Number of Flares per Sunspot Zurich Class\")\n",
    "ax2.set_xlabel(\"Modified Zurich Class\")\n",
    "ax2.set_ylabel(\"Average Number of Flares per Class Instance\")\n",
    "ax2.legend(title=\"Flare Type\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Multi-dimensional Risk Analysis\n",
    "\n",
    "This analysis examines how combinations of `largest spot size` and `spot distribution` patterns can contribute to flare risk, providing insights into the physical characteristics that drive solar flare activity.\n",
    "\n",
    "**Key Findings from 3D Risk Analysis:**\n",
    "\n",
    "Some combinations have lower sample size for reliable assessment. However, a general risk escalation from small, dispersed (X-X) to large, compact (K-C) configurations can be seen. Large, compact spot configurations (K-C, K-I combinations) show the highest risk scores, confirming that both `largest spot size` and `spot distribution` are critical factors, with their interaction creating non-linear risk patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish category order for plotting\n",
    "spot_sizes = category_orders[\"largest spot size\"]\n",
    "distributions = category_orders[\"spot distribution\"]\n",
    "\n",
    "# Create meshgrids for 3D plotting\n",
    "X_grid, Y_grid = np.meshgrid(spot_sizes, distributions)\n",
    "\n",
    "# Calculate average flare risk score for each combination\n",
    "Z_grid = np.zeros_like(X_grid, dtype=float)\n",
    "count_grid = np.zeros_like(X_grid, dtype=int)\n",
    "for i, spot_size in enumerate(spot_sizes):\n",
    "    for j, distribution in enumerate(distributions):\n",
    "        # Use exact matching for both spot size and distribution\n",
    "        mask = (df[\"largest spot size\"] == spot_size) & (\n",
    "            df[\"spot distribution\"] == distribution\n",
    "        )\n",
    "        count = mask.sum()\n",
    "        count_grid[j, i] = count\n",
    "        if count > 0:\n",
    "            risk_scores = (\n",
    "                df.loc[mask, \"common flares\"].fillna(0) * 1\n",
    "                + df.loc[mask, \"moderate flares\"].fillna(0) * 2\n",
    "                + df.loc[mask, \"severe flares\"].fillna(0) * 3\n",
    "            )\n",
    "            Z_grid[j, i] = risk_scores.mean()\n",
    "\n",
    "# Add the main risk surface\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Surface(\n",
    "            x=X_grid,\n",
    "            y=Y_grid,\n",
    "            z=Z_grid,\n",
    "            customdata=np.stack((X_grid.T, Y_grid.T, count_grid.T), axis=-1),\n",
    "            colorscale=\"Reds\",\n",
    "            hovertemplate=\"<b>Spot Size: %{x}<br>\"\n",
    "            + \"<b>Distribution: %{y}<br>\"\n",
    "            + \"<b>Average Flare Risk: %{z:.2f}<br>\"\n",
    "            + \"<b>Sample Size: %{customdata[2]}<extra></extra>\",\n",
    "            opacity=0.9,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.update_layout(\n",
    "    title=\"3D Surface: Flare Risk by Spot Size and Distribution\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Largest Spot Size\",\n",
    "        yaxis_title=\"Spot Distribution\",\n",
    "        zaxis=dict(\n",
    "            title=\"Risk Score\",\n",
    "            showticklabels=False,\n",
    "        ),\n",
    "        camera=dict(eye=dict(x=-1.5, y=-2, z=1.5)),\n",
    "    ),\n",
    "    height=700,\n",
    "    width=1000,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5lilJikbqfH"
   },
   "source": [
    "### 3.1. Feature Engineering\n",
    "\n",
    "The dataset tracks C, M, and X class flares in three separate columns, representing the count of each event type. For this classification task, a single target variable is needed. A new column will be created called `flare_class` that categorizes each sunspot region by the most significant flare it has produced.\n",
    "\n",
    "The original flare columns are dropped to prevent data leakage. This step ensures that the model will be trained on features that are predictive rather than features that contain information about the target variable itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bApKu4oVVaPe"
   },
   "outputs": [],
   "source": [
    "# Determine the highest flare class for each row\n",
    "def get_flare_class(row):\n",
    "    if row[\"severe flares\"] > 0:\n",
    "        return 3  # X-class\n",
    "    elif row[\"moderate flares\"] > 0:\n",
    "        return 2  # M-class\n",
    "    elif row[\"common flares\"] > 0:\n",
    "        return 1  # C-class\n",
    "    else:\n",
    "        return 0  # None\n",
    "\n",
    "\n",
    "# Create a new target column\n",
    "df[\"flare_class\"] = df.apply(get_flare_class, axis=1)\n",
    "\n",
    "# Drop original flare columns to prevent data leakage\n",
    "df.drop(columns=[\"common flares\", \"moderate flares\", \"severe flares\"], inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Data Preprocessing\n",
    "\n",
    "The dataset requires preprocessing to prepare it for machine learning algorithms. This preprocessing phase is critical for solar flare prediction because the success of our model depends heavily on how well the categorical and ordinal features are transformed into numerical representations that preserve their inherent relationships and meaning.\n",
    "\n",
    "**Key Preprocessing Steps:**\n",
    "\n",
    "- **Ordinal Encoding:** `largest spot size` and `spot distribution` are converted to numerical scales (1-6 and 1-4 respectively) that preserve their inherent ordering from least to most large/compact.\n",
    "\n",
    "- **Binary Feature Standardization:** Five features are binary and converted to standard 0/1 encoding. This follows ML best practices and ensures intuitive interpretation where higher values indicate greater complexity or size:\n",
    "  - `historically-complex` and `became complex on this pass`: 0 = \"no\" (not complex), 1 = \"yes\" (complex)\n",
    "  - `activity`: 0 = \"decay\", 1 = \"no change\"\n",
    "  - `area` and `area of largest spot`: 0 = smaller size/area, 1 = larger size/area\n",
    "\n",
    "- **One-Hot Encoding:** The `modified Zurich class` feature is transformed using one-hot encoding because of their nominal nature (H-class is decayed state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for each ordinal feature\n",
    "largest_spot_size_order = {\"X\": 1, \"R\": 2, \"S\": 3, \"A\": 4, \"H\": 5, \"K\": 6}\n",
    "spot_distribution_order = {\"X\": 1, \"O\": 2, \"I\": 3, \"C\": 4}\n",
    "\n",
    "# Map the string categories to their ordinal values\n",
    "df[\"largest spot size\"] = df[\"largest spot size\"].map(largest_spot_size_order)\n",
    "df[\"spot distribution\"] = df[\"spot distribution\"].map(spot_distribution_order)\n",
    "\n",
    "# Convert all binary categorical features to standard 0/1 encoding\n",
    "df[\"historically-complex\"] = (df[\"historically-complex\"] == 1).astype(\n",
    "    int\n",
    ")  # 0=no, 1=yes\n",
    "df[\"became complex on this pass\"] = (df[\"became complex on this pass\"] == 1).astype(\n",
    "    int\n",
    ")  # 0=no, 1=yes\n",
    "df[\"activity\"] = (df[\"activity\"] == 2).astype(int)  # 0=decay, 1=no change\n",
    "df[\"area\"] = (df[\"area\"] == 2).astype(int)  # 0=small, 1=large\n",
    "df[\"area of largest spot\"] = (df[\"area of largest spot\"] == 2).astype(\n",
    "    int\n",
    ")  # 0=<=5, 1=>5\n",
    "\n",
    "# One-hot encode the modified Zurich class feature\n",
    "categorical_cols = [\"modified Zurich class\"]\n",
    "df_encoded = pd.get_dummies(df, columns=[\"modified Zurich class\"])\n",
    "\n",
    "# Display the first few rows of the encoded dataframe\n",
    "print(\"Dataset shape:\", df_encoded.shape)\n",
    "display(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Correlation Matrix Heatmap\n",
    "\n",
    "Understanding feature relationships is crucial for solar flare prediction model performance and scientific interpretation. The correlation matrix reveals which sunspot characteristics work together to create flare-prone conditions and help us identify redundant features.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "- **Feature Independence Assessment:** The correlation analysis demonstrates that most feature relationships are not overly strong (<0.7), indicating minimal multicollinearity concerns for our machine learning models. This validates that our feature set maintains appropriate independence, reducing the risk of redundant information that could negatively impact model performance.\n",
    "\n",
    "- **Target Variable Insights:** The correlation analysis reveals distinct patterns in flare class prediction characteristics.\n",
    "\n",
    "  - **Positive correlations:** `spot distribution` (0.34), `largest spot size` (0.30), and `area` (0.26) emerge as the most predictive features.\n",
    "  - **Moderate correlations:** Activity-related metrics including `activity` (0.24) and `previous 24 hour flare activity` (0.19).\n",
    "  - **Modified Zurich Class:** `modified Zurich class` `D`, `E`, and `F` show high correlation, while `B`, `C`, and `H` show low to no correlation. This aligns with section 2.4.2, that more complex Zurich class regions are higher risk for flare production. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig = px.imshow(\n",
    "    df_encoded.corr(),\n",
    "    title=\"Correlation Matrix Heatmap\",\n",
    "    color_continuous_scale=\"RdBu_r\",\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    labels=dict(color=\"correlation\")\n",
    ")\n",
    "\n",
    "fig.update_layout(width=800, height=600, xaxis=dict(tickangle=-45))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Data Splitting\n",
    "\n",
    "The data is split into a 70-30 ratio which provides sufficient training data for our models while reserving a holdout set for performance assessment.\n",
    "\n",
    "A stratified split is used to ensure proportional representation of all flare classes across training and test sets. Given the extreme rarity of X-class flares, random splitting could result in test sets with zero X-class events, making performance evaluation impossible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df_encoded.drop(columns=[\"flare_class\"])\n",
    "y = df_encoded[\"flare_class\"]\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"No Flare\", \"C-Class\", \"M-Class\", \"X-Class\"]\n",
    "\n",
    "# Display distributions after stratified split\n",
    "print(\"Class Distribution:\")\n",
    "class_dist_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Original\": y.value_counts().sort_index().values,\n",
    "        \"Train\": y_train.value_counts().sort_index().values,\n",
    "        \"Test\": y_test.value_counts().sort_index().values,\n",
    "    },\n",
    "    index=class_names,\n",
    ")\n",
    "display(class_dist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Sampling Strategy\n",
    "\n",
    "This subsection implements sampling techniques specifically designed to improve X-class flare detection performance. Because of the extreme class imbalance in solar flare data, we want to prevent the models from simply predicting \"no flare\" for every case and achieving 85% accuracy.\n",
    "\n",
    "**SMOTEENN:**\n",
    "\n",
    "Our sampling strategy employs SMOTEENN (Synthetic Minority Oversampling Technique + Edited Nearest Neighbors), a method that addresses both the minority class shortage and majority class noise. This technique first generates synthetic examples of rare flare events using SMOTE (oversampling), then removes noisy majority class samples using ENN (undersampling).\n",
    "\n",
    "The Random Forest and XGBoost models benefit significantly from balanced training data, as each tree in the ensemble can learn meaningful patterns from minority classes rather than being overwhelmed by the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample training data using SMOTEENN to balance the classes\n",
    "sampler = SMOTEENN(random_state=42)\n",
    "X_train_sampled, y_train_sampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Calculate class counts and amplification\n",
    "final_counts = pd.Series(y_train_sampled).value_counts().sort_index()\n",
    "amplification = X_train_sampled.shape[0] / X_train.shape[0]\n",
    "\n",
    "# Print amplification\n",
    "print(\n",
    "    f\"Original shape: {X_train.shape}, Sampled shape: {X_train_sampled.shape}, Amplification: {amplification:.1f}x\"\n",
    ")\n",
    "\n",
    "# Bar plot for class distribution after sampling\n",
    "plt.bar(class_names, final_counts)\n",
    "plt.title(\"Class Distribution After Sampling (SMOTEENN)\")\n",
    "plt.ylabel(\"Sample Count\")\n",
    "plt.xlabel(\"Flare Class\")\n",
    "for i, v in enumerate(final_counts):\n",
    "    plt.text(i, v + 5, f\"{v} ({v / len(y_train_sampled) * 100:.1f}%)\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 4.1. Model Selection\n",
    "\n",
    "For solar flare prediction, we use three proven machine learning models:\n",
    "\n",
    "- **Random Forest**: An ensemble of decision trees, resistant to overfitting and useful for feature importance.\n",
    "- **XGBoost**: A high-performance gradient boosting method, effective for imbalanced and structured data.\n",
    "- **Support Vector Machine (SVM)**: Finds optimal class boundaries and works well with class weighting.\n",
    "\n",
    "These models are chosen for their strong performance on multi-class, imbalanced problems and their ability to capture complex relationships in the data. We will compare their results to select the best approach for predicting C, M, and X-class solar flares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter optimization is critical for our models' success, particularly given the extreme class imbalance and high-stakes nature of missing severe flare events. The tuning utilizes either grid search or randomized search, selecting the most appropriate method based on computational complexity and parameter space size.\n",
    "\n",
    "**Optimization Strategy:**\n",
    "\n",
    "- **Stratified 5-fold Cross-Validation:** This technique divides the training dataset into 5 equal folds while preserving the proportional distribution of each flare class (No Flare, C-Class, M-Class, X-Class) across all folds. Regular k-fold cross-validation could create folds with zero X-class examples, making validation impossible. Stratified sampling ensures each fold contains representative samples of all classes, providing reliable performance estimates and preventing overly optimistic results from class-imbalanced test sets.\n",
    "\n",
    "- **Class Balancing:** This strategy assigns different weights to each class during model training. Without class balancing, machine learning algorithms naturally bias toward predicting the majority class to minimize overall error, effectively ignoring rare events. We implement class weighting techniques that penalize misclassification of minority classes more heavily. This forces the model to pay equal attention to detecting rare X-class flares as it does to common cases, dramatically improving recall for severe events.\n",
    "\n",
    "- **Scoring:** F1 macro score gives equal weight to all classes regardless of their frequency. This is essential for solar flare prediction because standard accuracy metrics would be dominated by the 85% \"no flare\" cases, potentially hiding poor performance on critical X-class flares. F1 macro ensures that the model's ability to detect rare X-class events receives equal consideration to predicting common events, aligning with our business objective where missing severe flares has far greater consequences than false alarms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for Random Forest\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [10, 25, 50, 100],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"min_samples_split\": [10, 20, 50],\n",
    "    \"min_samples_leaf\": [5, 10, 20],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"class_weight\": [\"balanced\", \"balanced_subsample\", None],\n",
    "}\n",
    "\n",
    "# Set up grid search with cross-validation\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "\n",
    "# Fit Random Forest grid search on sampled training data\n",
    "rf_grid_search.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Print best cross-validation score and parameters for Random Forest\n",
    "print(f\"RF Best CV Score: {rf_grid_search.best_score_:.4f}\")\n",
    "print(f\"RF Best Params: {rf_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for XGBoost\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(y_train_sampled), y=y_train_sampled\n",
    ")\n",
    "sample_weights = np.array([class_weights[y] for y in y_train_sampled])\n",
    "\n",
    "# Hyperparameter grid for XGBoost\n",
    "xgb_grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"max_depth\": [1, 2, 3],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"min_child_weight\": [1, 5, 10, 20],\n",
    "    \"subsample\": [0.9, 1],\n",
    "    \"colsample_bytree\": [0.9, 1],\n",
    "    \"gamma\": [5.0, 10.0],\n",
    "    \"reg_alpha\": [0.05, 0.1, 0.5, 1.0, 2.0],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0, 5.0],\n",
    "}\n",
    "\n",
    "# Set up randomized search with cross-validation\n",
    "xgb_grid_search = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        tree_method=\"hist\",\n",
    "    ),\n",
    "    param_distributions=xgb_grid,\n",
    "    n_iter=50,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit XGBoost grid search on sampled training data\n",
    "xgb_grid_search.fit(\n",
    "    X_train_sampled,\n",
    "    y_train_sampled,\n",
    "    sample_weight=sample_weights,\n",
    ")\n",
    "\n",
    "# Print best cross-validation score and parameters for XGBoost\n",
    "print(f\"Best XGBoost parameters: {xgb_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1-macro score: {xgb_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training and test data for SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for SVM\n",
    "svm_grid = {\n",
    "    \"C\": [0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "    \"kernel\": [\"rbf\", \"linear\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"tol\": [1e-3, 1e-4],\n",
    "    \"max_iter\": [1000],\n",
    "}\n",
    "\n",
    "# Set up randomized search with cross-validation\n",
    "svm_grid_search = RandomizedSearchCV(\n",
    "    estimator=SVC(probability=True, random_state=42),\n",
    "    param_distributions=svm_grid,\n",
    "    n_iter=20,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=\"f1_macro\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit SVM grid search on sampled training data\n",
    "svm_grid_search.fit(X_train_scaled, y_train_sampled)\n",
    "\n",
    "# Print best cross-validation score and parameters for SVM\n",
    "print(f\"Best SVM parameters: {svm_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation F1-macro score: {svm_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Model Training\n",
    "\n",
    "All models are trained on the SMOTEENN-balanced dataset to ensure adequate representation of rare X-class flares while maintaining data quality through noise reduction. Each algorithm uses its optimized hyperparameters from the cross-validation process, with training performance metrics calculated to establish baseline expectations before final evaluation on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate metrics\n",
    "def get_metrics(y_true, y_pred, y_proba):\n",
    "    metrics = {}\n",
    "    metrics[\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"F1-macro\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    metrics[\"Recall-macro\"] = recall_score(y_true, y_pred, average=\"macro\")\n",
    "    metrics[\"Precision-macro\"] = precision_score(y_true, y_pred, average=\"macro\")\n",
    "    metrics[\"M-class Recall\"] = recall_score(y_true, y_pred, average=None)[2]\n",
    "    metrics[\"M-class Precision\"] = precision_score(y_true, y_pred, average=None)[2]\n",
    "    metrics[\"X-class Recall\"] = recall_score(y_true, y_pred, average=None)[3]\n",
    "    metrics[\"X-class Precision\"] = precision_score(y_true, y_pred, average=None)[3]\n",
    "    # ROC-AUC (macro)\n",
    "    y_true_bin = label_binarize(y_true, classes=[0, 1, 2, 3])\n",
    "    metrics[\"ROC-AUC-macro\"] = roc_auc_score(\n",
    "        y_true_bin, y_proba, average=\"macro\", multi_class=\"ovr\"\n",
    "    )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Define function to print metrics\n",
    "def print_metrics(metrics):\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"F1-macro: {metrics['F1-macro']:.4f}\")\n",
    "    print(f\"Recall-macro: {metrics['Recall-macro']:.4f}\")\n",
    "    print(f\"Precision-macro: {metrics['Precision-macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Make predictions on training set for initial assessment\n",
    "rf_train_pred = rf_model.predict(X_train_sampled)\n",
    "rf_train_proba = rf_model.predict_proba(X_train_sampled)\n",
    "\n",
    "# Calculate training metrics\n",
    "rf_train_metrics = get_metrics(y_train_sampled, rf_train_pred, rf_train_proba)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Random Forest training metrics:\")\n",
    "print_metrics(rf_train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 4.3.2. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb_grid_search.best_estimator_\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Make predictions on training set for initial assessment\n",
    "xgb_train_pred = xgb_model.predict(X_train_sampled)\n",
    "xgb_train_proba = xgb_model.predict_proba(X_train_sampled)\n",
    "\n",
    "# Calculate training metrics\n",
    "xgb_train_metrics = get_metrics(y_train_sampled, xgb_train_pred, xgb_train_proba)\n",
    "\n",
    "print(\"XGBoost training metrics:\")\n",
    "print_metrics(xgb_train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 4.3.3. Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm_grid_search.best_estimator_\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_scaled, y_train_sampled)\n",
    "\n",
    "# Make predictions on training set for initial assessment\n",
    "svm_train_pred = svm_model.predict(X_train_scaled)\n",
    "svm_train_proba = svm_model.predict_proba(X_train_scaled)\n",
    "\n",
    "# Calculate training metrics\n",
    "svm_train_metrics = get_metrics(y_train_sampled, svm_train_pred, svm_train_proba)\n",
    "\n",
    "print(\"SVM training metrics:\")\n",
    "print_metrics(svm_train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Performance on Holdout Set\n",
    "\n",
    "The holdout test set evaluation shows a moderate performance. All models achieved modest F1-macro scores, with Random Forest leading at 0.471, followed by SVM (0.436) and XGBoost (0.393).\n",
    "\n",
    "**Overfitting Analysis**\n",
    "\n",
    "A significant performance drop between training and test sets indicates overfitting across all models. The training vs. test F1-macro scores reveal a ~50% performance decrease. This suggests that our models learned specific patterns from the SMOTEENN-augmented training data that don't generalize well to real-world solar observations.\n",
    "\n",
    "The limited dataset size (1,389 observations) is also a factor in the overfitting present in the models. While SMOTEENN helped address class imbalance for training, the models may have memorized synthetic patterns rather than learning generalizable solar physics relationships.\n",
    "\n",
    "**Challenges and Model Limitations**\n",
    "\n",
    "All three models struggle particularly with M-class flare prediction, achieving poor recall rates and precision scores. This consistent weakness across algorithms suggests that M-class flares may represent the most challenging prediction category. They occur more frequently than X-class events but lack the extreme characteristics that make X-class flares more predictable.\n",
    "\n",
    "**Model Selection**\n",
    "\n",
    "Random Forest becomes the optimal choice through an analysis of the performance metrics. While SVM achieves the highest raw accuracy (76.5%), this metric is not as important in our highly imbalanced dataset where 85% of cases involve no flares. Based on business needs and success criteria, recall scores for X-class flares is the most important factor to consider.\n",
    "\n",
    "Random Forest leads the models with 75% recall for X-class flares, directly addressing our primary business objective where missing a single X-class flare could result in the highest amounts of infrastructure damage. To add to this, Random Forest maintains the highest F1-macro score (0.471) and ROC-AUC score (0.806)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all models\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "rf_test_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "xgb_test_proba = xgb_model.predict_proba(X_test)\n",
    "\n",
    "svm_test_pred = svm_model.predict(X_test_scaled)\n",
    "svm_test_proba = svm_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Collect evaluation metrics for each model\n",
    "results = {}\n",
    "results[\"Random Forest\"] = get_metrics(y_test, rf_test_pred, rf_test_proba)\n",
    "results[\"XGBoost\"] = get_metrics(y_test, xgb_test_pred, xgb_test_proba)\n",
    "results[\"SVM (Scaled)\"] = get_metrics(y_test, svm_test_pred, svm_test_proba)\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "# Display the summary table\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Confusion Matrix Analysis\n",
    "\n",
    "The normalized confusion matrices reveal patterns in model behavior and provide deeper insights into the classification challenges across different flare types. These visualizations help validate our model selection rationale.\n",
    "\n",
    "**Consistent Strengths Across Models**\n",
    "\n",
    "All three models demonstrate robust performance for \"No Flare\" predictions, with correct classification rates ranging from 81-84%. This consistency reflects the models' ability to reliably identify inactive sunspot regions, which represents the majority class and provides value for ruling out false alarms.\n",
    "\n",
    "**M-Class and X-Class Predictions**\n",
    "\n",
    "The confusion matrices again highlight challenges with M-class predictions (17-33% correct classification, 39-50% misclassified as \"No Flare\"), suggesting M-class flares lack distinctive features from inactive regions. For X-class events, Random Forest excels with 75% correct classification and no confusion with other flare classes, while XGBoost misclassifies 25% as M-class and SVM shows the poorest performance with X-class flares equally distributed across all categories (25% each).\n",
    "\n",
    "**Class Transition Confusion Patterns**\n",
    "\n",
    "The matrices show some confusion between adjacent flare classes, particularly C-class events being misclassified as either \"No Flare\" (32-36%) or M-class (9-14%). This gradient confusion pattern suggests that flare intensity exists on a spectrum where boundaries between classes are not always distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model predictions and class names\n",
    "model_preds = {\n",
    "    \"Random Forest\": rf_test_pred,\n",
    "    \"XGBoost\": xgb_test_pred,\n",
    "    \"SVM (Scaled)\": svm_test_pred,\n",
    "}\n",
    "model_names = list(model_preds.keys())\n",
    "\n",
    "# Create subplots for confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"Normalized Confusion Matrices\", fontsize=16)\n",
    "\n",
    "# Plot normalized confusion matrix for each model\n",
    "for i, (name, y_pred) in enumerate(model_preds.items()):\n",
    "    cmatrix = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cmatrix, display_labels=class_names)\n",
    "    disp.plot(ax=axes[i], cmap=\"Blues\", colorbar=True, values_format=\".2f\")\n",
    "    axes[i].set_title(f\"{name}\")\n",
    "    axes[i].set_xlabel(\"Predicted Label\")\n",
    "    axes[i].set_ylabel(\"True Label\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. ROC-AUC Analysis\n",
    "\n",
    "The ROC curves provide a comprehensive view of each model's discrimination ability across all flare classes, reinforcing Random Forest's superiority for mission-critical predictions. The Area Under the Curve (AUC) scores quantify each model's ability to distinguish between classes, with higher values indicating better discrimination capability.\n",
    "\n",
    "**Random Forest's Performance**\n",
    "\n",
    "Random Forest excels in predicting X-class (AUC=0.89) and M-class (AUC=0.70) flares, outperforming both XGBoost and SVM for severe solar events that threaten infrastructure. It also maintains a strong performance for \"No Flare\" (AUC=0.82) and C-class events (AUC=0.81). This balanced capability across all flare intensities makes Random Forest the optimal model for reliable space weather forecasting across the full spectrum of solar activity.\n",
    "\n",
    "**Validation of Model Selection**\n",
    "\n",
    "The ROC analysis confirms Random Forest as the optimal choice for operational deployment. The model's superior performance on M-class (0.70 vs 0.63 vs 0.60) and X-class (0.89 vs 0.75 vs 0.79) predictions directly aligns with our business objectives, where accurate detection of moderate to severe flares provides the greatest operational value for space weather forecasting and infrastructure protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output for multiclass ROC\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Create ROC curves for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "model_data = [\n",
    "    (\"Random Forest\", rf_test_proba, cm.Blues),\n",
    "    (\"XGBoost\", xgb_test_proba, cm.Reds),\n",
    "    (\"SVM (Scaled)\", svm_test_proba, cm.Greens),\n",
    "]\n",
    "\n",
    "# Calculate and plot ROC curves for each model\n",
    "for idx, (model_name, y_score, cmap) in enumerate(model_data):\n",
    "    ax = axes[idx]\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        ax.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            lw=2,\n",
    "            color=cmap(0.4 + (0.6 * (i + 1) / n_classes)),\n",
    "            label=f\"{class_names[i]} (AUC={roc_auc:.2f})\",\n",
    "        )\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.0])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(f\"{model_name} ROC Curves\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Model Interpretation\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) analysis provides crucial insights into how our Random Forest model makes X-class flare predictions. This interpretability helps us understand the physical relationships the model has learned from sunspot data.\n",
    "\n",
    "**Key Feature Drivers for X-Class Flare Prediction**\n",
    "\n",
    "The SHAP analysis confirms that `largest spot size` is the single most important predictor for X-class flares, with larger spots (high feature values, shown in red) consistently driving predictions toward higher X-class probability.\n",
    "\n",
    "The `historically-complex` feature shows strong influence, where historically complex regions decrease X-class prediction probability. This suggests that historically complex regions may have already released their stored magnetic energy through previous flare events, leaving them in a lower risk state.\n",
    "\n",
    "The `spot distribution` characteristic shows that compact configurations (high values) generally contribute positively to X-class predictions, while dispersed configurations (low values) reduce X-class likelihood. This confirms the physical intuition that concentrated magnetic energy in compact sunspot groups creates conditions favorable for explosive flare events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP explainer using our trained Random Forest model\n",
    "explainer = shap.Explainer(rf_model)\n",
    "\n",
    "# Calculate SHAP values specifically for X-class flare predictions (index 3)\n",
    "shap_values = explainer(X_test)[:, :, 3]\n",
    "\n",
    "# Generate a beeswarm plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "plt.title(\n",
    "    \"Feature Importance for X-Class Flare Prediction\\n(SHAP Analysis)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "plt.xlabel(\"SHAP Value (Impact on X-Class Flare Prediction)\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 6.1. Interactive Solar Flare Prediction System\n",
    "\n",
    "**Instructions:** Select the characteristics that apply to your sunspot group and click the \"Predict Flare Risk\" button to get a prediction.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "1. **Modified Zurich class:** Magnetic complexity (B=simple  F=complex, H=decayed)\n",
    "2. **Largest spot size:** Size of largest spot (X=smallest  K=largest)\n",
    "3. **Spot distribution:** Compactness (X=dispersed  C=compact)\n",
    "4. **Activity:** Recent growth (1=decay, 2=no change)\n",
    "5. **Evolution:** 24-hour evolution (1=decay, 2=no growth, 3=growth)\n",
    "6. **Previous 24 hour flare activity:** Prior flare activity (1=none, 2=one M1, 3=>one M1)\n",
    "7. **Historically complex:** Ever historically complex (1=Yes, 2=No)\n",
    "8. **Became complex on this pass:** Became complex on current transit (1=Yes, 2=No)\n",
    "9. **Area:** Total sunspot group area (1=small, 2=large)\n",
    "10. **Area of largest spot:** Largest individual spot area (1=5, 2=>5)\n",
    "\n",
    "**Risk Levels:** No Flare (Low)  C-Class (Moderate)  M-Class (High)  X-Class (Critical)\n",
    "\n",
    "**Example High-Risk Scenario:** Try setting Zurich Class to 'F', Largest Spot Size to 'K', Spot Distribution to 'C', and Previous 24 hour flare activity to 'One M1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create widgets for each feature\n",
    "zurich_widget = widgets.SelectionSlider(\n",
    "    options=category_orders[\"modified Zurich class\"],\n",
    "    value=\"C\",\n",
    "    description=\"Zurich Class:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"300px\"},\n",
    ")\n",
    "\n",
    "spot_size_widget = widgets.SelectionSlider(\n",
    "    options=category_orders[\"largest spot size\"],\n",
    "    value=\"R\",\n",
    "    description=\"Largest Spot Size:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"300px\"},\n",
    ")\n",
    "\n",
    "distribution_widget = widgets.SelectionSlider(\n",
    "    options=category_orders[\"spot distribution\"],\n",
    "    value=\"O\",\n",
    "    description=\"Spot Distribution:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"300px\"},\n",
    ")\n",
    "\n",
    "evolution_widget = widgets.SelectionSlider(\n",
    "    options=[\"Decay\", \"No Growth\", \"Growth\"],\n",
    "    value=\"No Growth\",\n",
    "    description=\"Evolution:\",\n",
    "    disabled=False,\n",
    "    layout={\"width\": \"300px\"},\n",
    ")\n",
    "\n",
    "flare_activity_widget = widgets.SelectionSlider(\n",
    "    options=[\"None\", \"One M1\", \">One M1\"],\n",
    "    value=\"None\",\n",
    "    description=\"Previous 24h Flare:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"300px\"},\n",
    ")\n",
    "\n",
    "activity_widget = widgets.RadioButtons(\n",
    "    options=[\"Decay\", \"No Change\"],\n",
    "    value=\"No Change\",\n",
    "    description=\"Activity:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "historically_complex_widget = widgets.RadioButtons(\n",
    "    options=[\"No\", \"Yes\"],\n",
    "    value=\"No\",\n",
    "    description=\"Historically Complex:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "became_complex_widget = widgets.RadioButtons(\n",
    "    options=[\"No\", \"Yes\"],\n",
    "    value=\"No\",\n",
    "    description=\"Became Complex:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "area_widget = widgets.RadioButtons(\n",
    "    options=[\"Small\", \"Large\"],\n",
    "    value=\"Small\",\n",
    "    description=\"Area:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "spot_area_widget = widgets.RadioButtons(\n",
    "    options=[\"5\", \">5\"],\n",
    "    value=\"5\",\n",
    "    description=\"Largest Spot Area:\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "# Prediction button and output\n",
    "predict_button = widgets.Button(\n",
    "    description=\"Predict Flare Risk\",\n",
    "    button_style=\"primary\",\n",
    "    tooltip=\"Click to make prediction\",\n",
    ")\n",
    "\n",
    "output_widget = widgets.HTML(\n",
    "    value='<div style=\"padding: 10px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9;\">Select sunspot characteristics above and click \"Predict Flare Risk\" to get a prediction.</div>'\n",
    ")\n",
    "\n",
    "\n",
    "# Function to prepare input data for model\n",
    "def prepare_input_data():\n",
    "    # Map categorical values to numerical\n",
    "    zurich_order = {\"B\": 0, \"C\": 1, \"D\": 2, \"E\": 3, \"F\": 4, \"H\": 5}\n",
    "\n",
    "    # Create input array\n",
    "    input_data = np.zeros(15)  # 15 features total after preprocessing\n",
    "\n",
    "    # Set ordinal features\n",
    "    input_data[0] = largest_spot_size_order[spot_size_widget.value]\n",
    "    input_data[1] = spot_distribution_order[distribution_widget.value]\n",
    "\n",
    "    # Set numerical features\n",
    "    input_data[2] = 1 if activity_widget.value == \"No Change\" else 0\n",
    "    input_data[3] = [\"Decay\", \"No Growth\", \"Growth\"].index(evolution_widget.value) + 1\n",
    "    input_data[4] = [\"None\", \"One M1\", \">One M1\"].index(flare_activity_widget.value) + 1\n",
    "    input_data[5] = 1 if historically_complex_widget.value == \"Yes\" else 0\n",
    "    input_data[6] = 1 if became_complex_widget.value == \"Yes\" else 0\n",
    "    input_data[7] = 1 if area_widget.value == \"Large\" else 0\n",
    "    input_data[8] = 1 if spot_area_widget.value == \">5\" else 0\n",
    "\n",
    "    # One-hot encode Zurich class (features 9-14)\n",
    "    zurich_idx = zurich_order[zurich_widget.value]\n",
    "    if zurich_idx < 6:\n",
    "        input_data[9 + zurich_idx] = 1\n",
    "\n",
    "    return input_data.reshape(1, -1)\n",
    "\n",
    "\n",
    "# Prediction function\n",
    "def make_prediction(button):\n",
    "    try:\n",
    "        # Prepare input data\n",
    "        input_data = prepare_input_data()\n",
    "\n",
    "        # Make prediction using Random Forest\n",
    "        prediction = rf_model.predict(input_data)[0]\n",
    "        probabilities = rf_model.predict_proba(input_data)[0]\n",
    "\n",
    "        # Map prediction to class names\n",
    "        predicted_class = class_names[prediction]\n",
    "        confidence = probabilities[prediction] * 100\n",
    "\n",
    "        # Create color coding for different risk levels\n",
    "        if prediction == 0:\n",
    "            color = \"#28a745\"  # Green for no flare\n",
    "            risk_level = \"LOW\"\n",
    "        elif prediction == 1:\n",
    "            color = \"#ffc107\"  # Yellow for C-class\n",
    "            risk_level = \"MODERATE\"\n",
    "        elif prediction == 2:\n",
    "            color = \"#fd7e14\"  # Orange for M-class\n",
    "            risk_level = \"HIGH\"\n",
    "        else:  # X-class\n",
    "            color = \"#dc3545\"  # Red for X-class\n",
    "            risk_level = \"CRITICAL\"\n",
    "\n",
    "        # Format probability breakdown\n",
    "        prob_breakdown = \"<br>\".join(\n",
    "            [\n",
    "                f\"{class_names[i]}: {probabilities[i] * 100:.1f}%\"\n",
    "                for i in range(len(class_names))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create formatted output\n",
    "        output_html = f\"\"\"\n",
    "        <div style=\"padding: 15px; border: 2px solid {color}; border-radius: 8px; background-color: #fff;\">\n",
    "            <h3 style=\"color: {color}; margin-top: 0;\">Prediction Results</h3>\n",
    "            <div style=\"font-size: 18px; margin-bottom: 10px;\">\n",
    "                <strong>Predicted Class:</strong> <span style=\"color: {color}; font-weight: bold;\">{predicted_class}</span>\n",
    "            </div>\n",
    "            <div style=\"font-size: 16px; margin-bottom: 10px;\">\n",
    "                <strong>Risk Level:</strong> <span style=\"color: {color}; font-weight: bold;\">{risk_level}</span>\n",
    "            </div>\n",
    "            <div style=\"font-size: 14px; margin-bottom: 10px;\">\n",
    "                <strong>Confidence:</strong> {confidence:.1f}%\n",
    "            </div>\n",
    "            <div style=\"font-size: 12px; color: #666;\">\n",
    "                <strong>Probability Breakdown:</strong><br>\n",
    "                {prob_breakdown}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "        output_widget.value = output_html\n",
    "\n",
    "    except Exception as e:\n",
    "        output_widget.value = f'<div style=\"padding: 10px; border: 1px solid #dc3545; border-radius: 5px; background-color: #f8d7da; color: #721c24;\">Error making prediction: {str(e)}</div>'\n",
    "\n",
    "\n",
    "# Connect button to prediction function\n",
    "predict_button.on_click(make_prediction)\n",
    "\n",
    "# Create layout with each widget on its own line for better clarity\n",
    "physics_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            '<h4 style=\"margin-bottom: 4px; color: #2c3e50;\">Magnetic and Physical Characteristics</h4>'\n",
    "        ),\n",
    "        zurich_widget,\n",
    "        widgets.HTML('<div style=\"margin: 4px 0;\"></div>'),\n",
    "        spot_size_widget,\n",
    "        widgets.HTML('<div style=\"margin: 4px 0;\"></div>'),\n",
    "        distribution_widget,\n",
    "    ],\n",
    "    layout={\"width\": \"600px\"},\n",
    ")\n",
    "\n",
    "area_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            '<h4 style=\"margin-bottom: 4px; color: #2c3e50;\">Area Measurements</h4>'\n",
    "        ),\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                area_widget,\n",
    "                spot_area_widget,\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "activity_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            '<h4 style=\"margin-bottom: 8x; color: #2c3e50;\">Recent Activity and Evolution</h4>'\n",
    "        ),\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                activity_widget,\n",
    "                widgets.VBox(\n",
    "                    [\n",
    "                        evolution_widget,\n",
    "                        widgets.HTML('<div style=\"margin: 4px 0;\"></div>'),\n",
    "                        flare_activity_widget,\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "complexity_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML(\n",
    "            '<h4 style=\"margin-bottom: 4px; color: #2c3e50;\">Complexity Indicators</h4>'\n",
    "        ),\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                historically_complex_widget,\n",
    "                became_complex_widget,\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prediction_section = widgets.VBox(\n",
    "    [predict_button, widgets.HTML('<div style=\"margin: 4px 0;\"></div>'), output_widget]\n",
    ")\n",
    "\n",
    "# Main interface layout with better spacing\n",
    "main_interface = widgets.VBox(\n",
    "    [\n",
    "        widgets.HBox(\n",
    "            [\n",
    "                physics_section,\n",
    "                area_section,\n",
    "            ]\n",
    "        ),\n",
    "        activity_section,\n",
    "        complexity_section,\n",
    "        prediction_section,\n",
    "    ],\n",
    "    layout={\"padding\": \"20px\", \"max_width\": \"800px\"},\n",
    ")\n",
    "\n",
    "# Display the interface\n",
    "display(HTML('<div style=\"padding: 10px; background: purple; border-radius: 10px; text-align: center; max-width: 800px;\"><h2 style=\"color: white; margin: 0;\">Solar Flare Prediction System</h2></div>'))\n",
    "display(main_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 6.2. Deployment Recommendations\n",
    "\n",
    "For operational deployment, the Random Forest model can be implemented through cloud infrastructure with auto-scaling capabilities to handle varying demands during different solar cycle phases. Integration with existing space weather forecasting centers would enable real-time predictions through API endpoints.\n",
    "\n",
    "**Phased Implementation:**\n",
    "- **Phase 1:** Advisory predictions alongside existing methods\n",
    "- **Phase 2:** Primary operational deployment with human oversight\n",
    "- **Phase 3:** Automated alerting with configurable thresholds\n",
    "\n",
    "The system should include continuous monitoring for model performance drift and provide retraining capabilities as new solar data becomes available. Operational staff will require training on model interpretation and confidence intervals to effectively use the prediction outputs for critical infrastructure protection decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Conclusions\n",
    "\n",
    "This machine learning analysis successfully developed solar flare prediction models using a dataset of 1,389 sunspot observations. Despite significant data challenges, we achieved meaningful predictive capability with our final Random Forest model.\n",
    "\n",
    "**Model Performance Results:**\n",
    "\n",
    "Our Random Forest model achieved the strongest overall performance with:\n",
    "- **Macro F1-score: 0.471** (best among all three models tested)\n",
    "- **X-class flare recall: 75%** (meeting our critical success criterion of >60%)\n",
    "- **Overall accuracy: 76.5%** on the holdout test set\n",
    "- **ROC-AUC: 0.806** indicating strong discrimination capability\n",
    "\n",
    "**Key Technical Challenges:**\n",
    "\n",
    "The most prominent challenges were the small dataset size of only 1,389 total observations (further reduced by train/test splitting) and the extreme class imbalance with X-class flares representing merely 1% of cases. The combination of these two factors meant our models had insufficient examples of X-class flares (14 total cases) to learn predictive patterns.\n",
    "\n",
    "Additional challenges included:\n",
    "- M-class flare prediction difficulty (17-33% correct classification across models)\n",
    "- Limited feature set with only 10 input variables\n",
    "\n",
    "**Model Selection Rationale:**\n",
    "\n",
    "Random Forest was selected as the optimal model based on:\n",
    "1. **Superior X-class recall (75%)** - critical for minimizing missed severe events\n",
    "2. **Highest macro F1-score (0.471)** - balanced performance across all classes  \n",
    "3. **Strong feature interpretability** through SHAP analysis\n",
    "4. **Robust ROC-AUC performance** for both M-class (0.70) and X-class (0.89) predictions\n",
    "\n",
    "**Key Feature Insights:**\n",
    "\n",
    "SHAP analysis revealed that `largest spot size` is the most important predictor for X-class flares, followed by `spot distribution` (compact configurations increase risk) and `historically-complex` status (surprisingly shows inverse relationship).\n",
    "\n",
    "**Technical Limitations:**\n",
    "\n",
    "While achieving our primary success criteria, the model's moderate F1-macro score (0.471) indicates substantial room for improvement. The consistent difficulty across all algorithms in predicting M-class flares suggests these events may lack distinctive characteristics in our current feature set, representing the most challenging prediction category in solar flare prediction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPDdiR6kQYo2jRCqiFc+7x",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
