{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sinahuss/solar-flare-prediction/blob/main/notebooks/solar_flare_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkTkq0gHjNlg"
   },
   "source": [
    "# C964 Capstone: Solar Flare Prediction and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### Organizational Need\n",
    "\n",
    "Space weather events, particularly solar flares, pose significant risks to critical infrastructure on Earth and in space. Organizations like NOAA's Space Weather Prediction Center require reliable early warning systems to protect:\n",
    "\n",
    "- Satellite communications and GPS systems\n",
    "- Power grids\n",
    "- Astronauts and aircraft\n",
    "- Radio communications\n",
    "\n",
    "Current prediction methods rely heavily on human expertise and limited historical patterns, which may result in missed events or false alarms. These risks can lead to potentially billions of dollars in economic damage and disruptions to essential services.\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "This project aims to develop a data product featuring a machine learning model that can predict the likelihood of solar flare events (C, M, or X class) within a 24-hour period based on characteristics of sunspot regions. The model will provide early warning capability for space weather forecasters, and improved accuracy in flare prediction to reduce false alarms and missed events.\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "The model's success will be measured by:\n",
    "\n",
    "- High recall for M and X class flares (the most dangerous events) to minimize missed warnings\n",
    "- Balanced precision and recall to reduce false alarms while maintaining sensitivity\n",
    "- Practical deployment feasibility for integration into existing space weather monitoring systems\n",
    "\n",
    "This predictive capability would enable space weather agencies to provide more reliable warnings, allowing for better preparation and protection of critical infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-2r8JEfeptP"
   },
   "source": [
    "### 2.1. Load Libraries and Data\n",
    "\n",
    "Our solar flare prediction analysis begins with importing essential libraries and loading the sunspot dataset.\n",
    "\n",
    "The dataset will be loaded from a public GitHub repository containing the Solar Flare Dataset from Kaggle, which provides the historical data needed to train our flare prediction model.\n",
    "\n",
    "This dataset contains morphological characteristics of sunspot groups that solar physicists use to assess flare potential. The first few rows will be displayed to verify successful data loading and provide an initial glimpse of the sunspot characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCBbIizzO1Mu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Load dataset from GitHub repository (public Kaggle dataset)\n",
    "url = \"https://raw.githubusercontent.com/sinahuss/solar-flare-prediction/refs/heads/main/data/data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display first few rows to verify successful loading\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dataset Feature Descriptions\n",
    "\n",
    "The dataset contains 13 features describing each solar active region. The first 10 are the input features for our model, and the last three are the target variables we aim to predict.\n",
    "\n",
    "**Input Features:**\n",
    "\n",
    "- **modified Zurich class:** A classification of the sunspot group's magnetic complexity, generally ordered from least to most complex (A, B, C, D, E, F, H).\n",
    "- **largest spot size:** Size of the largest spot in the group, ordered from smallest to largest (X, R, S, A, H, K).\n",
    "- **spot distribution:** Compactness of the sunspot group, ordered from least to most compact (X, O, I, C).\n",
    "- **activity:** A code representing the region's recent growth (1=decay, 2=no change).\n",
    "- **evolution:** Describes the region's evolution over the last 24 hours (1=decay, 2=no growth, 3=growth).\n",
    "- **previous 24 hour flare activity:** A code summarizing prior flare activity (1=none, 2=one M1, 3=>one M1).\n",
    "- **historically-complex:** A flag indicating if the region was ever historically complex (1=Yes, 2=No).\n",
    "- **became complex on this pass:** A flag indicating if the region became complex on its current transit (1=Yes, 2=No).\n",
    "- **area:** A code for the total area of the sunspot group (1=small, 2=large).\n",
    "- **area of largest spot:** A code for the area of the largest individual spot (1=<=5, 2=>5).\n",
    "\n",
    "**Target Variables:**\n",
    "\n",
    "- **common flares:** The number of C-class flares produced in the next 24 hours.\n",
    "- **moderate flares:** The number of M-class flares produced in the next 24 hours.\n",
    "- **severe flares:** The number of X-class flares produced in the next 24 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRY4P9xGI94V"
   },
   "source": [
    "### 2.3. Initial Data Inspection\n",
    "\n",
    "A foundational understanding of the dataset's structure and quality must be established. This inspection is critical for the solar flare prediction model because data quality directly impacts model performance and reliability for space weather forecasting.\n",
    "\n",
    "First, we will use `.info()` to examine the column names, data types, and check for any missing values. The output confirms that there are no missing values, meaning that null values do not have to be accounted for in the data preparation phase.\n",
    "\n",
    "Next, we use `describe()` to generate a summary of the categorical features, including their unique values and most frequent entries, which helps us understand the distribution and composition of the dataset's categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmC34kT4I-sO"
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "df.astype(\"object\").describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1. Target Variable Analysis\n",
    "\n",
    "Before analyzing the input features, we must first understand the distribution of our target variables: `common flares`, `moderate flares`, and `severe flares`. The plots below show the number of 24-hour periods in the dataset that recorded zero, one, two, or more flares of each type.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "The visualization reveals a severe class imbalance for our solar flare prediction. Out of all 24-hour periods available, only 15% experienced at least one C-Class event, 5% recorded M-Class events, and 1% showed X-Class events.\n",
    "\n",
    "This imbalance has several implications for our machine learning approach:\n",
    "\n",
    "1. **Model Selection:** Traditional accuracy metrics will be misleading due to the dominance of the \"no flare\" class, so there should be higher focus on precision, recall, and F1-score.\n",
    "\n",
    "2. **Sampling Strategy:** We may need to employ techniques like stratified sampling to address the imbalance.\n",
    "\n",
    "3. **Evaluation Metrics:** The model's success will be measured primarily by its ability to correctly identify the rare but dangerous M and X-class flares, rather than overall accuracy.\n",
    "\n",
    "4. **Business Impact:** Missing an X-class flare (false negative) is far more costly than incorrectly predicting one (false positive), making recall for severe flares our primary optimization target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flare_columns = [\"common flares\", \"moderate flares\", \"severe flares\"]\n",
    "\n",
    "# Create a figure with 3 subplots, one for each flare type\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "fig.suptitle(\"Distribution of Raw Flare Counts Per 24-Hour Period\")\n",
    "\n",
    "# Loop through each flare type and plot its distribution\n",
    "for i, col in enumerate(flare_columns):\n",
    "    ax = axes[i]\n",
    "    countplot = sns.countplot(\n",
    "        data=df, x=col, ax=ax, hue=col, palette=\"viridis\", legend=False\n",
    "    )\n",
    "    ax.set_title(f\"Distribution of {col}\")\n",
    "    ax.set_xlabel(\"Flares Recorded\")\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%d\", label_type=\"edge\", padding=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Relationship Analysis\n",
    "\n",
    "We now explore the relationship between flare production and modified Zurich class through two visualizations. These analyses investigate a key hypothesis: that more complex sunspot groups produce more significant flares.\n",
    "\n",
    "**Total Flares Analysis:** The first subplot shows the total number of C, M, and X-class flares produced by each modified Zurich class, revealing which sunspot configurations are the most prolific sources of solar flares.\n",
    "\n",
    "**Average Flares Analysis:** The second subplot normalizes this data by showing the average number of flares per class instance, accounting for the different frequencies of each modified Zurich class in the dataset. This provides a more accurate assessment of flare risk per sunspot group.\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "The two visualizations help us prioritize which modified Zurich class to observe.\n",
    "\n",
    "- **Low-Risk:** B and C class sunspot regions are low complexity and produce the least amount of solar flares, so they can be seen as low-risk regions. H class regions are decayed remnants of C, D, E, and F regions, and are also low-risk regions.\n",
    "\n",
    "- **Medium-Risk:** D class sunspot regions are interesting because they produce the highest number of total solar flares in the dataset. But, after normalizing the data, we can see that they actually produce significantly fewer flares per sunspot region. Therefore, they can be categorized as medium-risk regions.\n",
    "\n",
    "- **High-Risk:** E class regions are almost guaranteed to produce solar flares, reaching just under 1 C-Class solar flare per instance. F class regions produce a low total amount of solar flares, but adjusting for their lower representation in the dataset, they produce a high number of solar flares per region. F class regions also produce the highest amount of X-class (severe) flares when data is normalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to have a single column for flare type and another for the count\n",
    "flare_counts_df = df.melt(\n",
    "    id_vars=[\"modified Zurich class\"],\n",
    "    value_vars=[\"common flares\", \"moderate flares\", \"severe flares\"],\n",
    "    var_name=\"flare_type\",\n",
    "    value_name=\"count\",\n",
    ")\n",
    "\n",
    "# Specify the order for each categorical feature for consistent plotting\n",
    "category_orders = {\n",
    "    \"modified Zurich class\": [\"B\", \"C\", \"D\", \"E\", \"F\", \"H\"],\n",
    "    \"largest spot size\": [\"X\", \"R\", \"S\", \"A\", \"H\", \"K\"],\n",
    "    \"spot distribution\": [\"X\", \"O\", \"I\", \"C\"],\n",
    "}\n",
    "\n",
    "# Remove rows where flares have not occurred\n",
    "flare_counts_df = flare_counts_df[flare_counts_df[\"count\"] > 0]\n",
    "\n",
    "# Calculate the number of sunspot groups for each Zurich class\n",
    "zurich_class_counts = df[\"modified Zurich class\"].value_counts().to_dict()\n",
    "\n",
    "# Calculate the proportional number of flares (per Zurich class instance)\n",
    "flare_counts_df[\"class_count\"] = flare_counts_df[\"modified Zurich class\"].map(\n",
    "    zurich_class_counts\n",
    ")\n",
    "flare_counts_df[\"count_per_class\"] = (\n",
    "    flare_counts_df[\"count\"] / flare_counts_df[\"class_count\"]\n",
    ")\n",
    "\n",
    "# Create a figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Create the Grouped Bar Plot\n",
    "sns.barplot(\n",
    "    data=flare_counts_df,\n",
    "    x=\"modified Zurich class\",\n",
    "    y=\"count\",\n",
    "    hue=\"flare_type\",\n",
    "    estimator=sum,\n",
    "    order=category_orders[\"modified Zurich class\"],\n",
    "    palette=\"viridis\",\n",
    "    errorbar=None,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Total Flares Produced by Sunspot Zurich Class\")\n",
    "ax1.set_xlabel(\"Modified Zurich Class\")\n",
    "ax1.set_ylabel(\"Total Number of Flares Recorded\")\n",
    "ax1.legend(title=\"Flare Type\")\n",
    "\n",
    "# Second subplot: Average Flares per Class Instance\n",
    "sns.barplot(\n",
    "    data=flare_counts_df,\n",
    "    x=\"modified Zurich class\",\n",
    "    y=\"count_per_class\",\n",
    "    hue=\"flare_type\",\n",
    "    estimator=sum,\n",
    "    order=category_orders[\"modified Zurich class\"],\n",
    "    palette=\"viridis\",\n",
    "    errorbar=None,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Average Number of Flares per Sunspot Zurich Class\")\n",
    "ax2.set_xlabel(\"Modified Zurich Class\")\n",
    "ax2.set_ylabel(\"Average Number of Flares per Class Instance\")\n",
    "ax2.legend(title=\"Flare Type\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5lilJikbqfH"
   },
   "source": [
    "### 3.1. Feature Engineering\n",
    "\n",
    "The dataset tracks C, M, and X class flares in three separate columns, representing the count of each event type. For this classification task, a single target variable is needed. A new column will be created called `flare_class` that categorizes each sunspot region by the most significant flare it has produced in the following 24-hour period. The values 0, 1, 2, and 3 correspond to 'None', 'C', 'M', and 'X' class flares, respectively.\n",
    "\n",
    "The original flare columns are dropped to prevent data leakage. This step ensures that the model will be trained on features that are predictive rather than features that contain information about the target variable itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bApKu4oVVaPe"
   },
   "outputs": [],
   "source": [
    "# Determine the highest flare class for each row\n",
    "def get_flare_class(row):\n",
    "    if row[\"severe flares\"] > 0:\n",
    "        return 3  # X-class\n",
    "    elif row[\"moderate flares\"] > 0:\n",
    "        return 2  # M-class\n",
    "    elif row[\"common flares\"] > 0:\n",
    "        return 1  # C-class\n",
    "    else:\n",
    "        return 0  # None\n",
    "\n",
    "\n",
    "# Create a new target column\n",
    "df[\"flare_class\"] = df.apply(get_flare_class, axis=1)\n",
    "\n",
    "print(df[\"flare_class\"].value_counts())\n",
    "\n",
    "# Drop original flare columns to prevent data leakage\n",
    "df.drop(columns=[\"common flares\", \"moderate flares\", \"severe flares\"], inplace=True)\n",
    "\n",
    "# Display the first few rows to see the new columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo0In97iiQSt"
   },
   "source": [
    "### 3.2. Data Preprocessing\n",
    "\n",
    "The raw sunspot data requires preprocessing to prepare it for machine learning algorithms. This preprocessing phase is critical for solar flare prediction because the success of our model depends heavily on how well the categorical and ordinal features are transformed into numerical representations that preserve their inherent relationships and physical meaning.\n",
    "\n",
    "**Key Preprocessing Steps:**\n",
    "\n",
    "- **Ordinal Encoding:** `largest spot size` and `spot distribution` are converted to numerical scales (1-6 and 1-4 respectively) that preserve their inherent ordering from least to most complex/energetic.\n",
    "\n",
    "- **Binary Feature Standardization:** Five features are identified as binary and converted to standard 0/1 encoding:\n",
    "  - `historically-complex` and `became complex on this pass`: 0 = \"no\" (not complex), 1 = \"yes\" (complex)\n",
    "  - `activity`: 0 = \"decay\", 1 = \"no change\"\n",
    "  - `area` and `area of largest spot`: 0 = smaller size/area, 1 = larger size/area\n",
    "  \n",
    "  This follows ML best practices and ensures intuitive interpretation where higher values indicate greater complexity or size.\n",
    "\n",
    "- **One-Hot Encoding:** The `modified Zurich class` feature is transformed using one-hot encoding to prevent the algorithm from inferring incorrect ordinal relationships between the different sunspot classifications.\n",
    "\n",
    "This preprocessing approach optimizes compatibility with machine learning algorithms. Ordinal relationships are preserved and binary features are clearly interpretable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0I9Vi67biM2k"
   },
   "outputs": [],
   "source": [
    "# Define the order for each ordinal feature\n",
    "largest_spot_size_order = {\"X\": 1, \"R\": 2, \"S\": 3, \"A\": 4, \"H\": 5, \"K\": 6}\n",
    "spot_distribution_order = {\"X\": 1, \"O\": 2, \"I\": 3, \"C\": 4}\n",
    "\n",
    "# Map the string categories to their ordinal values\n",
    "df[\"largest spot size\"] = df[\"largest spot size\"].map(largest_spot_size_order)\n",
    "df[\"spot distribution\"] = df[\"spot distribution\"].map(spot_distribution_order)\n",
    "\n",
    "# Convert all binary categorical features to standard 0/1 encoding\n",
    "df[\"historically-complex\"] = (df[\"historically-complex\"] == 1).astype(int)  # 0=no, 1=yes\n",
    "df[\"became complex on this pass\"] = (df[\"became complex on this pass\"] == 1).astype(int)  # 0=no, 1=yes\n",
    "df[\"activity\"] = (df[\"activity\"] == 2).astype(int)  # 0=decay, 1=no change\n",
    "df[\"area\"] = (df[\"area\"] == 2).astype(int)  # 0=small, 1=large\n",
    "df[\"area of largest spot\"] = (df[\"area of largest spot\"] == 2).astype(int)  # 0=<=5, 1=>5\n",
    "\n",
    "# One-hot encode the modified Zurich class feature\n",
    "categorical_cols = [\"modified Zurich class\"]\n",
    "df_encoded = pd.get_dummies(df, columns=[\"modified Zurich class\"])\n",
    "\n",
    "print(\"Dataset shape:\", df_encoded.shape)\n",
    "print(\"\\nBinary feature distributions:\")\n",
    "binary_features = [\"historically-complex\", \"became complex on this pass\", \"activity\", \"area\", \"area of largest spot\"]\n",
    "for feature in binary_features:\n",
    "    print(f\"{feature}: {df_encoded[feature].value_counts().to_dict()}\")\n",
    "\n",
    "df_encoded.head()\n",
    "\n",
    "# For SVM only - add after the encoding steps\n",
    "# scaler = sk.preprocessing.StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Feature Correlation Analysis\n",
    "\n",
    "To better understand the relationships between input features and the target variable, we visualize the correlation matrix of the fully encoded dataset. This analysis reveals critical insights about feature predictiveness.\n",
    "\n",
    "**Key Insights from Correlation Analysis:**\n",
    "\n",
    "**Target Variable Relationships (`flare_class`):**\n",
    "The correlation heatmap reveals specific patterns in how sunspot characteristics relate to flare production:\n",
    "\n",
    "- **Strong Positive Predictors:**\n",
    "  - `modified Zurich class` `D`, `E`, and `F`: Complex magnetic configurations increase flare probability\n",
    "  - `spot distribution` and `largest spot size`: Classifications similar to `modified Zurich class` related to compactness and size, respectively\n",
    "  - `area`: Larger sunspot groups are more likely to produce higher-class flares\n",
    "  - `previous 24 hour flare activity`: Recent flare history predicts continued activity\n",
    "\n",
    "- **Negative Correlations:**\n",
    "  - `modified Zurich class` `B`, `C`, and `H`: Simple or decayed sunspot regions have reduced flare potential\n",
    "  - `historically-complex` and `became complex on this pass`: Complexity factors have negative correlations, which requires deeper understanding of the dataset and solar physics. The `historically-complex` feature shows negative correlation because most historically-complex regions decay to the H-class, which is highly represented in the dataset. The `became complex on this pass` feature may indicate that it takes longer than 24 hours after a region has become complex for it to produce solar flares.\n",
    "\n",
    "**Multicollinearity Assessment:**\n",
    "Most feature correlations remain within acceptable ranges (|r| < 0.7), indicating that our feature set provides complementary rather than redundant information. The strongest correlations are between logically related features, which is expected and manageable.\n",
    "\n",
    "**Physical Interpretation:**\n",
    "These findings align with solar physics understanding that:\n",
    "1. **Flare productivity peaks during active development phases** rather than historical complexity periods\n",
    "2. **Magnetic complexity evolution is temporal** - regions cycle through phases of growth, peak activity, and decay\n",
    "3. **Current morphological characteristics** (spot size, distribution, area) are more predictive of immediate flare potential than historical or developmental complexity flags\n",
    "\n",
    "**Implications for Model Development:**\n",
    "This analysis suggests our models should prioritize:\n",
    "- Current physical characteristics over historical complexity indicators\n",
    "- Active Zurich classes (D, E, F) rather than complexity flags\n",
    "- Recent activity patterns and morphological features as primary predictors\n",
    "\n",
    "This counterintuitive pattern actually strengthens confidence in our dataset's physical validity, as it captures the realistic temporal evolution of solar active regions from development through decay phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df_encoded.corr()\n",
    "\n",
    "# Set up the figure size\n",
    "plt.figure(figsize=(18, 14))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=False,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.9},\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation Heat Map of Encoded Features and Target\", fontsize=24)\n",
    "plt.xticks(fontsize=16, rotation=45, ha=\"right\")\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Target correlations\n",
    "target_corr = df_encoded.corr()['flare_class'].sort_values(ascending=False)\n",
    "print(\"Target correlations\")\n",
    "print(\"Top positive predictors:\")\n",
    "print(target_corr.head(10)[1:-1])\n",
    "print(\"\\nTop negative predictors:\")\n",
    "print(target_corr.tail(5)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Data Splitting\n",
    "\n",
    "The final step in data preparation is to split the dataset into training and testing sets. This separation is crucial for evaluating the model's performance on unseen data and preventing overfitting.\n",
    "\n",
    "**Key Considerations for Solar Flare Prediction:**\n",
    "\n",
    "Given the severe class imbalance identified in our exploratory analysis, stratified sampling should be employed to ensure both training and testing sets maintain the same proportion of flare classes. This is particularly critical for the rare but dangerous X-class flares, which represent only 1% of the dataset.\n",
    "\n",
    "**Split Strategy:**\n",
    "\n",
    "- **Training Set (80%):** Used to train the machine learning model\n",
    "- **Testing Set (20%):** Reserved for final model evaluation\n",
    "- **Stratified Sampling:** Maintains class distribution across both sets\n",
    "- **Random State:** Fixed for reproducibility of results\n",
    "\n",
    "The resulting datasets will be used to train and evaluate our classification model, with higher attention to the model's ability to detect M and X class flares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df_encoded.drop(columns=[\"flare_class\"])\n",
    "y = df_encoded[\"flare_class\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Using stratified sampling to maintain class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nClass distribution in testing set:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPDdiR6kQYo2jRCqiFc+7x",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
